{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f96907-5c43-4570-87d2-c7183d819a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sliderule import icesat2\n",
    "import sliderule\n",
    "import pyproj\n",
    "from scipy import linalg\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "sliderule.set_verbose(False)\n",
    "# this is needed in some environments, not in others:\n",
    "pyproj.datadir.set_data_dir('/home/jovyan/envs/wf_pointCollection/share/proj')\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51217030-9426-4bef-9c61-765d170dab93",
   "metadata": {},
   "source": [
    "# The workflow:\n",
    "\n",
    "This is a quick-and-dirty representation of how we would map height-change rates for a small region on the ice sheet.  The workflow could be scaled up to make a map of elevation-change rate for an ice sheet with the caveats that the data volume increases dramatically toward the poles and that fitting a single elevation-change rate does not take full advantage of the data.\n",
    "\n",
    "Workflow steps:\n",
    "\n",
    "1. Read all available ATL06 data for a 40x40-km box centered on x0, y0\n",
    "2. Filter the data for atl06_quality_summary==0\n",
    "3. Select data in each 1x1 km box in the dataset, fit the selected data with a vertically moving plane\n",
    "\n",
    "The remaining cells in the notebook plot the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cab185-c944-47a5-945a-70b9a9bd78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xy0 = [0., -3.0e6]\n",
    "crs=3413\n",
    "W=4.e4\n",
    "pad=np.array([-W/2, W/2])\n",
    "bbox = [xy0[0] + pad[[0, 1, 1, 0, 0]], xy0[1] + pad[[0, 0, 1, 1, 0]]]\n",
    "# generate a clipping polygon in a lat/lon coordinate system\n",
    "bbox_ll = pyproj.Transformer.from_crs(pyproj.CRS(crs), \n",
    "                                      pyproj.CRS(4326))\\\n",
    "                    .transform(*bbox)\n",
    "poly = [{\"lat\":lat, \"lon\":lon} for lat, lon in zip(*bbox_ll)]\n",
    "\n",
    "save_file = f'ATL06_E{int(xy0[0]/1.e3)}_N{int(xy0[1]/1.e3)}.parquet'\n",
    "icesat2.atl06sp(\n",
    "   {\n",
    "    \"poly\": poly,\n",
    "    \"t0\":\"2018-10-13T00:00:00Z\",\n",
    "    \"t1\":\"2025-10-13T00:00:00Z\", \n",
    "   \n",
    "    \"output\": {\n",
    "      \"format\": \"parquet\",\n",
    "      \"as_geo\": True,\n",
    "      \"path\": save_file,\n",
    "      \"with_checksum\": False\n",
    "    }})\n",
    "df=gpd.read_parquet(save_file).to_crs(3413)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14688a58-a1eb-440a-ab99-156aa573b7f1",
   "metadata": {},
   "source": [
    "# set a decimal-year time field, filter the data for quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df996f-5a21-4d3b-8289-9e57d3f6b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] =(np.array(df.index) - np.datetime64(datetime.datetime(2018, 1, 1))).astype(float)/1.e9/24/3600/365.25 +2018\n",
    "df=df[df.atl06_quality_summary==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa844a-bac8-4956-8464-2065519715b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over 1-km tiles in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90bbb0-72b7-403a-8c26-320192c81d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index_dict(data, bin_size):\n",
    "    xy0 = np.round(np.array(data.geometry.x+1j*data.geometry.y)/bin_size)*bin_size\n",
    "    u_ii, index, inverse=np.unique(xy0, return_index=True, return_inverse=True)\n",
    "    uXY=xy0[index]\n",
    "    \n",
    "    bin_dict={}\n",
    "    inv_arg_ind=np.argsort(inverse)\n",
    "    inv_sort=inverse[inv_arg_ind]\n",
    "    ind_delta=np.concatenate([[-1], np.where(np.diff(inv_sort))[0], [inv_sort.size]])\n",
    "    for ii in range(len(ind_delta)-1):\n",
    "        this_ind=inv_arg_ind[(ind_delta[ii]+1):(ind_delta[ii+1]+1)]\n",
    "        this_xy0 = xy0[this_ind[0]]\n",
    "        bin_dict[tuple([np.real(this_xy0), np.imag(this_xy0)])]=this_ind\n",
    "    return bin_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e51b9f-9208-430c-bff4-cb32c086028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function that fits a time-varying plane to the data for a selected set of points\n",
    "\n",
    "def dhdt_est(x, y, z, t, ind):\n",
    "    #xx=np.array(D.geometry.x[ind])\n",
    "    #yy=np.array(D.geometry.y[ind])\n",
    "    #zz=np.array(D.h_li[ind])\n",
    "    #tt =np.array(D.time[ind])\n",
    "    xx=x[ind]\n",
    "    yy=y[ind]\n",
    "    zz=z[ind]\n",
    "    tt=t[ind]\n",
    "    G=np.c_[np.ones(len(ind)), tt-2020, xx-np.mean(xx), yy-np.mean(yy)]\n",
    "    sigma = 1.e4\n",
    "    keep = np.ones(G.shape[0], dtype=bool)\n",
    "    keep_last = np.zeros_like(keep)\n",
    "    n_iterations=0\n",
    "    try:\n",
    "        # iterate fit to reduce outliers\n",
    "        while not np.all(keep_last==keep) and n_iterations  < 5:\n",
    "            n_iterations += 1\n",
    "            m=linalg.lstsq(G[keep,:], zz[keep])[0]\n",
    "            r=zz-G@m\n",
    "            sigma_r = np.nanstd(r[keep])\n",
    "            keep = np.abs(r) < 3*np.maximum(sigma, 0.1)\n",
    "        # estimate errors\n",
    "        G1 = G[keep,:]\n",
    "        Ginv = np.linalg.inv(G1.T@G1)@G1.T\n",
    "        Cm = Ginv@Ginv.T*(np.maximum(0.03**2, sigma_r**2))\n",
    "        sigma_dhdt = np.sqrt(Cm[1][1])\n",
    "        return m[1], np.sum(keep), sigma_r, sigma_dhdt\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd3323-1c5e-45d8-aee7-daa46e1cce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# apply the dh/dt function to each 1- km square in the data\n",
    "# my initial test of this passed the dataframe to the dhdt_est function, and extracted a subset from (e.g.) df.geometry.x\n",
    "# for each bin.  This was terribly slow.  Extracting x, y, z, and t to numpy arrays outside the loop made the results much faster.\n",
    "\n",
    "bin_dict=make_index_dict(df, 1.e3)\n",
    "xy_dhdt = []\n",
    "x=np.array(df.geometry.x)\n",
    "y=np.array(df.geometry.y)\n",
    "z=np.array(df.h_li)\n",
    "t=np.array(df.time)\n",
    "for xyc, ind in bin_dict.items():\n",
    "    dhdt, N, sigma_r, sigma_dhdt = dhdt_est(x, y, z, t, ind)\n",
    "    xy_dhdt += [{'geometry':shapely.Point(xyc),\n",
    "                 'dhdt':dhdt,\n",
    "                 'N':N,'sigma_r':sigma_r,\n",
    "                 'sigma_dhdt':sigma_dhdt}]\n",
    "    \n",
    "D_dhdt=gpd.GeoDataFrame(xy_dhdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863525cc-02c1-440b-a5a0-c7c68c90539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results:\n",
    "\n",
    "hf, hax=plt.subplots(1, 2, figsize=[8,4], layout='constrained', sharex=True, sharey=True)\n",
    "\n",
    "plt.sca(hax[0])\n",
    "small_σ= D_dhdt.sigma_dhdt < 0.125\n",
    "plt.plot(D_dhdt.geometry.x[small_σ==0], D_dhdt.geometry.y[small_σ==0],'.', color='darkgray')\n",
    "plt.colorbar(\n",
    "    plt.scatter(D_dhdt.geometry.x[small_σ], D_dhdt.geometry.y[small_σ], 3, c=D_dhdt.dhdt[small_σ], cmap='RdBu', clim=[-1, 1]), \n",
    "    label='dh/dt, m/yr', shrink=0.5, extend='both')\n",
    "plt.gca().set_facecolor('lightgray')\n",
    "plt.gca().set_title('dh/dt')\n",
    "\n",
    "\n",
    "plt.sca(hax[1])\n",
    "plt.colorbar(\n",
    "    plt.scatter(D_dhdt.geometry.x, D_dhdt.geometry.y, 3, c=D_dhdt.sigma_dhdt, cmap='magma', clim=[0, 1]), \n",
    "    label='dh/dt uncertainty, m/yr', shrink=0.5, extend='max')\n",
    "plt.gca().set_facecolor('lightgray')\n",
    "plt.gca().set_title('uncertainty')\n",
    "\n",
    "for ax in hax: \n",
    "    ax.set_aspect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659abeba-1c76-43cf-b8c2-8d8af75f5310",
   "metadata": {},
   "source": [
    "# Speed from test run:\n",
    "\n",
    "For xy0 = [0, -3.e6] in Greenland,\n",
    "* Fetching data: 28.8 s\n",
    "* Reading cache: 456 ms\n",
    "* Processing: 5 microseconds \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc14cb-a6ea-451f-858c-65dea16c97c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wf_pointCollection",
   "language": "python",
   "name": "wf_pointcollection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
